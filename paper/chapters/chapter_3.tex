%
\section{Image Tree Based on WordNet}
\label{sec_wordnetsearchtree}

The following chapter describes the creation of an image tree based on semantic information given by WordNet. An ontology is the ``explicit specification of a conceptualization'' \cite{gruber1995ontology}, therefore, characterizes entities and their relationships. How we use an ontology to detect semantic concepts represented by given images is explained in chapters \ref{sec_wordnet} and \ref{sec_keywordstopictures}. The image tree is built upon a given search term. The search term is used to create a search tree, see chapter \ref{sec_searchtreeconstruction} and then, images are assigned to nodes of the search tree, according to their detected semantic concepts, see chapter \ref{sec_picturestonodes}.

\subsection{The WordNet Ontology}\index{WordNet}
\label{sec_wordnet}
The official web page describes WordNet as a freely and publicly available ``large lexical database of English nouns, verbs, adjectives and adverbs, grouped into sets of cognitive synonyms (\emph{Synsets}\index{Synset}), each expressing a distinct concept''\footnote{http://wordnet.princeton.edu/}. That means, a Synset is a particular concept which can be expressed by different terms but has one unique identifier. The identifier consists of the word most commonly used to describe the concept, the part of speech, and a number, e.g. \emph{drive.v.02}.\\
The number is necessary because one word can have multiple meanings that will then be represented by different synsets, like in \emph{cherry.n.01} for the tree and \emph{cherry.n.02} for the fruit. All Synsets a certain term may represent can be obtained by calling \emph{wn.synsets(``term'')}. This call includes stemming the term, so its plural or conjugations will be matched as well.\\

Synsets are linked with each other through several semantic relations, e.g. \emph{part-of}, \emph{member-of} (meronyms) or \emph{type-of} (hyponyms) relationships.
In our work, we use this network of synsets to discover the semantics between terms describing the images as well as towards the search term. \\

Another popular ontology\index{Ontology} we could have used to explore semantic relationships is DBpedia\footnote{http://www.dbpedia.org/}, which is a Linked Data Project based on Wikipedia's infoboxes.
Compared to WordNet, DBpedia contains more information in terms of entities, relationships, and attributes, and has support for multiple languages. However, since it is open data, it is by far not as well-structured as WordNet, and 	often inconsistent or redundant.

\subsection{Assigning Keywords to Pictures}
\label{sec_keywordstopictures}
The first steps that had to be taken was to identify valuable image annotations, and to then find the terms' meanings in order to map them to the correct Synsets\index{Synset}.

\subsubsection{Annotation Data}
\label{sec_annotationdata}
We considered the following annotations provided by the Flickr API and evaluated them on twenty randomly sampled pictures:
\begin{itemize}
\item{\emph{Title.}} The title was usually a short but precise description of the image content and thus very valuable for semantic annotation.
\item{\emph{Description.}} The description did often relate to the image content but with a lot of fill words and noise as well as context-dependent meanings, so it could be useful but would require additional preprocessing such as Named Entity Recognition.
\item{\emph{Comments.}} Only very few comments described the image in any way - they were mostly used for social interaction with the photographer.
\item{\emph{Tags.}} Tags are short, precise keywords on various abstraction levels. The vast majority of them are directly related to the image contents, and only little noise present due to the absence of fill words.
\item{\emph{Album Names.}} There are albums for diverse purposes, many of them related to the images' contents. Their names, however, tend to be obscured with special characters and the like, so quite some effort would be necessary in preprocessing.
\item{\emph{Group Names.}} The observations on group names were similar to those on albums.
\end{itemize}

Based on these findings, we use the single words from the title (split by whitespace) as well as tags. Before trying to find their corresponding Synsets, the keywords are cleansed: All those including digits are removed, since they more often represent image metadata (such as camera model, lens width, date, etc.) than information on the image contents. Additionally, all remaining keywords are stripped of special characters to achieve a more uniform representation. An endless number of additional filters could be introduced to avoid matching errors, but it must also be considered that potentially valuable information will also be removed by these filters.\\

\subsubsection{Synset Detection}
\label{sec_synsetdetection}
The difficulty in assigning Synsets\index{Synset} to images is that there are multiple possible Synsets for a word, and it is obvious to a human observer but not to a computer which meaning is correct. Assuming that annotations on each image are closely related because they describe the same image content, we use those Synsets that, altogether, give the smallest semantic distance across all annotations of an image. Semantic distance of two terms can be measured by the length of the path between them in the WordNet tree. We use the Leacock and Chodorow Normalized Path Length (LCH-Similarity)\index{LCH-Similarity} provided by WordNet, which uses adapted weights and normalization factors, because it is perceived as closer to human understanding than regular path similarity \cite{budanitsky01}. \\

To efficiently find the set of Synsets with the smallest overall distance, a best-first search algorithm \footnote{Please refer to Artificial Intelligence literature, i.e. \cite{kumar2008} for a detailed explanation.} is used. Note that such search algorithms require non-negative distances between options, but WordNet provides similarities. To convert them into distances without changing the scale, the similarity is simply subtracted from the maximally possible similarity, i.e. the similarity of a Synset to itself, which is roughly 3.7.
For complexity reasons, only the best 100 candidates are considered at any time. Of course, this does not guarantee the perfect result anymore, but other paths are highly unlikely to become the best candidate in the end, and keeping all candidates would decrease performance significantly. \\
We also limit the matching to nouns, for two reasons: First, nouns are usually the words describing the depicted concepts. Second, the LCH-Similarity described above is only available within a part of speech. \todo{reference?}

This strategy provides decent results, although erroneous matching still occurs. One cause are words that are meant in a way that is unknown to WordNet, i.e. canon as the camera model might be interpreted as the type of music piece. Another cause are adjectives, adverbs and verbs that also exist in a noun form. The most common cause of this effect are pictures tagged with colors, because most terms describing a color also exist as nouns, like ``orange'' for the fruit, or ``white'' for a Caucasian person. We decided to add a filter to the preprocessing phase, so that all terms that can represent a color are removed. \\

Even with preprocessing, not all keywords can be matched to a Synset, because they are simply not represented in WordNet. The information about these \emph{unmatched tags} is kept nevertheless, and later used for image retrieval, described in section \ref{sec_picturestonodes}.


\subsection{Constructing a Searchtree}
\label{sec_searchtreeconstruction}
In general, all words represented in WordNet can be used as a query term for our tool. \todo{Find better description of what search terms we expect} For the given use case, however, queries will be limited to those that can be seen in pictures. This is mainly the case for object descriptors at various levels of specificity, and place names, so our work is focused on these types of search terms.

When a term is entered into the tool, it is first used to retrieve all Synsets\index{Synset} that can be expressed by this term. For each of them, a separate searchtree is constructed, as can be seen in Figure \ref{fig_searchtree}, showing excerpts of the searchtrees for ``bird'' (\emph{bird.n.01} and \emph{bird.n.02}).

\begin{figure}[h]
\includegraphics[width=\textwidth]{images/searchtree.pdf}
\caption{Examplary searchtree (excerpt) for search term ``bird''}
\label{fig_searchtree}
\end{figure}

The same figure also visualizes that a searchtree is a tree of specializations. These specializations are retrieved using WordNet's hypnoym relations\index{Hyponym}. For some terms, especially geographic Synsets, specializations are not applicable, so we use part-meronyms\index{Part-meronym} (part-of relationships), when no hyponyms are available.\\


Figure \ref{fig_nodestructure} shows the internal data structure of the tree. Each node represents one Synset, and references a list of more specific Synsets (\emph{hyponyms} or \emph{meronyms}).

\begin{figure}[h]
\centering
\includegraphics[]{images/wordnetnode_class_diagram.pdf}
\caption{Tree node data structure}
\label{fig_nodestructure}
\end{figure}


\subsection{Assigning Pictures to Tree Nodes}
\label{sec_picturestonodes}
Generally, assigning pictures to tree nodes is simple: Each node gets linked with all images that have been annotated with the Synset it represents during Synset detection. \\
In addition, strongly co-occurring tags are used for a higher recall. Co-occurring tags are those keywords that could not be matched to any Synset. They may, however, be closely related to certain Synsets, with which they often occur together. When that is the case, the keyword is added to the list of \emph{co\_occurring\_tags} of the node. \\
We define a \emph{strong} co-occurrence based on term frequency and inverse document frequency (tf-idf) \index{Tf-idf} values. The term frequency describes a normalized frequency of a term $t_i$ in a document $d_i$:
\[tf(t_i,d_i) = \frac{freq(t_i,d_i)}{\max_k freq(f_k,d_i)}\]
In our case a Synset is regarded as document and an unmatched keyword as the term. The $freq(t_i,d_i)$ is the number of co-occurrences of that unmatched keyword and Synset. On the contrary, the inverse document frequency describes the inverted frequency of a term over all documents:
\[idf(t_i, D) = \log \frac{\vert D\vert}{\vert \{d \in D; t_i \text{ occurs in }d\}\vert} \text{, with } D = \{d_1,..., d_n\}\]
Unmatched keywords and Synsets again are regarded as terms and documents. Therefore, idf has a high value if an unmapped keyword occurs only with few Synsets. It is considered to be more important for the Synset than for example a camera model which occurs with many Synsets. Tf and idf are multiplied to consider both the number of occurrences with a certain Synset and the number of overall occurrences.
 If a simpler co-occurrence measure (e.g. the ratio of co-occurrences to the total number of occurrences of the term) was used, very common keywords like camera models would be strong co-occurrences with many Synsets despite the lack of an actual relation. \\
We observed that the co-occurring keywords can be useful to find terms in foreign languages and proper nouns, but of course also introduces noise. The key to the quality of this features is the choice of the threshold. Reasonably good results were achieved with $0.75 * max\_tf\_idf$, where $max\_tf\_idf$ is the maximal score across all values. \\

After adding all pictures that are annotated with the Synset itself or one of the related tags to the node's \emph{associated\_pictures}, some nodes may only have one or very few images. To create a balanced result with image sets of a significant size, nodes considered too small are merged into their parent node. Whether a node is too small is determined by the parameter \emph{minimal\_node\_size}, which states the minimal number of images a node must have. To avoid merging of small nodes completely, the parameter should be set to zero.\\
The merge process is simple: All associated pictures of the node are combined with the parent node's pictures via union. Existing subnodes are not modified.

\bigskip
The above described steps of the Image Tree Creation phase are summarized in figure \ref{fig_imagetreecreation}.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{images/image_tree_creation.pdf}
\caption{Process of Image Tree Creation}
\label{fig_imagetreecreation}
\end{figure}
