%
\section{Evaluation}
\label{sec_evaluation}

We evaluated our tool on a set of 9,201 images, which are a subset of the 1 million images of the MIRFLICKR-1M\footnote{http://press.liacs.nl/mirflickr/} file set, and the query term ``food''. Since no comparable algorithms exist, the evaluation is mainly aimed at obtaining the best values for the parameters and at providing a basis for comparison of further improvements and future work.

\subsection{Test set}
\label{sec_testset}
No gold standard is available to tell us which pictures show food and how similar the images are. The creation of such standards and training data is exactly the task we want to facilitate with this work.\\ 
To test the quality of our algorithm, we wrote a tool allowing us a crowdsourced generation of the needed reference data by the general public. This was achieved in two phases:\\

First, the users were shown random picture out of the 9,201 test set images and asked whether it shows food or not. We normalized these answers, so that there is only one vote per user per picture. In the case a user rated a picture multiple times, the value is determined by the ratio of positive (\emph{``shows food''}) and negative (\emph{``does not show food''}) votes of each user on one picture. We consider all those images as showing food that received at least 50\% positive votes. With over 35,000 clicks by more than 20 participants, 1,142 images out of the total 9,201 images were identified to show food. \\

We then also required data on the semantic and visual similarity of these pictures. Therefore, in the second phase, the users were shown pairs of images, on which we knew from phase 1 that they contain food, and asked to compare them. They could choose between three levels of semantic similarity: \emph{not similar}, \emph{same object}, and \emph{same object and same context}, and two levels of visual similarity: \emph{similar}, and \emph{not similar}.\\
Among the 12,962 votes of more than 30 participants were 757 pairs of images with same objects, 345 pairs with same object and same context, as well as 1,854 pairs of visually similar images. Multiple votes on one pair were rare (39 cases), and therefore simply not taken into account if they contradicted each other.

\subsection{Quality Indicators}
The evaluation focuses on the following four main aspects of our algorithm:
\begin{enumerate}
\item Retrieval of matching images
\item Semantic hierarchy and clusters
\item Visual clustering
\end{enumerate}

We measure the quality of the image retrieval (\emph{1.}) by calculating the F-Measure for the returned pictures, comparing our algorithm's result to the crowdsourced generated test set of phase 1. \todo{should we compare synset detection mechanisms?} 

The quality of the hierarchy of the retrieved images (\emph{2.}) is based on the \emph{same object} and \emph{same object and context} pairs: The  minimal path distance for an annotated pair of pictures can be calculated and used to determine the closeness of two images $closeness(x,y) = 1/distance(x,y)$. Averaging this value over all pairs of a similarity category returns a value between 0 and 1 (below referenced as $c_o$ for same object pairs, $c_c$ for same object and same context pairs, and $c_n$ for not similar pairs), with the optimal values being 1 for positive (similar) pairs, and 0 for negative (non-similar) pairs. 
We include the keyword clustering in this evaluation by handling the clusters in a node as its children. Consequentially, the perfect score of 1 can only be reached when two semantically similar images are not only in the same node but also in the same semantic subcluster.

Visual similarity (\emph{3.}) is evaluated on the whole test set, because not enough comparison data is available to get valuable results if only comparisons within semantic clusters were used. Once again, F-Measure is used as indicator.

\todo{vary parameters given by frontend, trying to find best configuration}

\subsection{Results}
\label{sec_results}

\subsubsection*{Image retrieval}

Our image retrieval has a precision of p = 50.2\% and recall of r = 85.9\% on the ``food'' query, before execution of the semantic clustering that removes outliers. Without the use of co-occurring tags described in section \ref{sec_picturestonodes}, both values show no significant difference with p = 50.5\% and r = 85.4\%.
After the semantic clustering, the measures depend on the \emph{minimal mcl cluster size} parameter. The results for different values of this parameter are presented in table \ref{tab_retrievalevaluation}.\\

\begin{table}[h]
   \begin{tabular}{| p{2.2cm}| p{2.2cm}| p{2cm} || p{2cm} | p{2cm} | p{2cm} |}
    \hline
    \emph{mcl clustering threshold} & \emph{minimal mcl cluster size} & \emph{minimal node size} & \emph{precision} & \emph{recall} & \emph{f-measure} \\ \hline
    0 	& 0 	& 0 & 0.501532 & 0.859143 & 0.633344 \\ \hline
    0 	& 5 	& 0 & 0.559668 & 0.783036 & 0.652773 \\ \hline
    5 	& 5 	& 5 & 0.549815 & 0.798214 & 0.651129 \\ \hline     
    15 	& 25 &  5 & 0.615894 & 0.747321 & 0.675272 \\ \hline
    15 	& 10 & 15 & 0.585333 & 0.783929 & 0.670229 \\ \hline
    15 	& 25 & 15 & 0.695298 & 0.672791 & 0.683859 \\ \hline
    	100 	& 100 & 100 & 0.757858 & 0.569554 & 0.650350 \\ \hline
    \end{tabular}
    \caption{Precision and recall of the image retrieval}
	\label{tab_retrievalevaluation}
\end{table}

\begin{table}[h]
    \begin{tabular}{| p{2cm}| p{2cm}| p{2cm} || p{2cm} | p{2cm} | p{2cm} |}
    \hline
    2	& 6	& 2	& 0.575916 & 0.769904 & 0.658929 \\ \hline    
    4	& 2	& 2	& 0.518719 & 0.836395 & 0.640322 \\ \hline	      
    4	& 2	& 4	& 0.520458 & 0.834646 & 0.641129 \\ \hline	
    4	& 6	& 2	& 0.562267 & 0.790026 & 0.656966 \\ \hline     
    4 	& 4 	& 4 & 0.550663 & 0.798775 & 0.651910 \\ \hline
 	4	& 6	& 4	& 0.568766 & 0.774278 & 0.655798 \\ \hline    
    6 	& 6 	& 6 & 0.567619 & 0.782152 & 0.657837 \\ \hline  
	5 	& 10 & 5 & 0.612903 & 0.748031 & 0.673759 \\ \hline
    5 	& 15 & 5 & 0.644391 & 0.708661 & 0.675000 \\ \hline  
   	5 	& 10 & 10 & 0.598080 & 0.762905 & 0.670511 \\ \hline 
    10 	& 10 & 10 & 0.598080 & 0.762905 & 0.670511 \\ \hline 
    10 	& 10 & 20 & 0.578165 & 0.783027 & 0.665180 \\ \hline    
    10 	& 20 & 20 & 0.642631 & 0.709536 & 0.674428 \\ \hline
    10 	& 15 & 10 & 0.631539 & 0.728784 & 0.676686 \\ \hline  
    10 	& 20 & 10 & 0.666102 & 0.687664 & 0.676711 \\ \hline
    15 	& 25 & 10 & 0.655201 & 0.714286 & 0.683469 \\ \hline
    15 	& 20 & 15 & 0.661716 & 0.701662 & 0.681104 \\ \hline    
    20 	& 20 & 20 & 0.642631 & 0.709536 & 0.674428 \\ \hline
    20 	& 25 & 20 & 0.677951 & 0.683290 & 0.680610 \\ \hline  
    50	& 50 & 50 & 0.728135 & 0.604549 & 0.660612 \\ \hline
    100 	& 100 & 100 & 0.757858 & 0.569554 & 0.650350 \\ \hline
    \end{tabular}
    \caption{Additional precision and recall of the image retrieval}
\end{table}


\begin{table}[h]
    \begin{tabular}{| p{1.8cm} | p{1.8cm} || p{1.5cm} | p{1.5cm} | p{1.5cm} |}
    \hline
    ?	& \emph{minimal node size} & $c_o $ & $c_c$ & $c_n$ \\ \hline
    value 1 		& value 1 	& value 1 	& value 1	& value 1 \\ \hline
    value 2 		& value 2	& value 2 	& value 2	& value 2 \\ \hline
    value 3	 	& value 3	& value 3 	& value 3	& value 3 \\
    \hline
    \end{tabular}
    \caption{Semantic quality measures}
	\label{tab_treeevaluation}
\end{table}

\subsubsection*{Semantic and visual clustering}

Within our second phase we let users decide which of the pictures declared as showing food are semantically and visually similar. 

Image tuples voted with same object: 757 \\
Image tuples voted with same object and same context: 1102 \\
Image tuples voted as visual similar: 1854 \\

After retrieving our evaluation votes from the users, we compared the result of our tool with one from the evaluation. Therefore we calculated the distance between every two images, which are declared as similar or not, within our calculated search tree. \todo{Picture of a search tree, which illustrates the evaluation script concerning how the distance is calculated?}

Average distance for same object  is 2.77091633466  \\
Average distance for same object and same context is 2.64246575342 \\
Average distance for not similar  is 2.87943462898 \\


