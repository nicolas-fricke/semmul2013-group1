%
\section{Evaluation}
\label{sec_literatur}

We evaluated our tool on a set of 9000 \todo{exact number?} images and the query term ``food''. Since no comparable algorithms exist, the evaluation is mainly aimed at obtaining the best values for the parameters and at providing a basis for comparison of further improvements and future work.\\

\subsection{Testset}
receive testset from users through web-based tool, crowdsource in two steps: \\

\begin{enumerate}
\item random picture, does it show food? identified 1270 images out of 9202 as showing food with over 35000 clicks, where those with at least 50 percent of positives votes are considered to show food (normalized so that one vote per evaluator, value is this persons relation of positive and negative votes on this pictures) \\
\item For those that are considered to show food, compare two pictures: semantically not similar / same object / same object and same context? visually similar / not similar?\\
\end{enumerate}

\subsection{Evaluation Method}
The evaluation focusses on the three main aspects of our algorithm:
\begin{enumerate}
\item Retrieval of matching images
\item Semantic hierarchy of retrieved images
\item Semantic clustering
\item Visual clustering
\end{enumerate}

We measure the quality of the image retrieval by precision and recall of returned pictures, compared to those that were declared to show food by the test persons. 

search food: precision  / recall of picture inclusion (compare synset detection mechanisms?) \\
evaluate tree nodes based on same object / same context annotations: average minimal path distance for annotated pairs of pictures, use to calculate similarity $1/distance$. Optimal: 1 for positive pairs, 0 for negative pairs\\
evaluate mcl clusters based on same object and on same context annotations (compare both, what does mcl actually do?)\todo{how do we actually evaluate mcls?} \\

We evaluate visual similarity on the whole testset, because not enough comparison data to get valuable results if we only use comparisons within semantic clusters. Calculate precision and recall\\

vary parameters given by frontend, trying to find best configuration \\

\subsection{Results}

\missingfigure{table: parameters, results }