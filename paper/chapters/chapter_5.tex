%
\section{Evaluation}
\label{sec_evaluation}

We evaluated our tool on a set of 9,201 images and the query term ``food''. The images are a subset of the 1 million images of the MIRFLICKR-1M\footnote{http://press.liacs.nl/mirflickr/} file set. Since no comparable algorithms exist, the evaluation is mainly aimed at obtaining the best values for the parameters and at providing a basis for comparison of further improvements and future work.

\subsection{Test Set Creation}
\label{sec_testset}
No gold standard is available to tell us which pictures show food and how similar the images are. The creation of such standards and training data is exactly the task we want to facilitate with this work.\\
To test the quality of our algorithm, we wrote a tool that allowed us to crowdsource the needed reference data from the general public. This was achieved in two phases:\\

First, the users were shown a random picture out of the 9,201 test set images and asked whether it shows food or not. The answers to this were normalized, as to have only one vote per user per picture. In the case a user rated a picture multiple times, the value is determined by the ratio of positive (\emph{``shows food''}) and negative (\emph{``does not show food''}) votes of each user on one picture. We consider all those images as showing food that received more than 50\% positive votes. With over 35,000 clicks by more than 20 participants, 1,142 images out of the total 9,201 images were identified to show food.\\

Since data on the semantic and visual similarity of these pictures is also necessary for the evaluation, a second phase was run on the images that had been claimed to contain food in the first phase. The users were shown pairs of those images and  asked to compare them. Users could choose between three levels of semantic similarity: \emph{not similar}, \emph{same object}, and \emph{same object and same context}, and two levels of visual similarity: \emph{similar}, and \emph{not similar}.\\
Among the 12,962 votes by more than 30 participants, were 757 pairs of images with same objects, 345 pairs with same object and same context, as well as 1,854 pairs of visually similar images. Multiple votes on one pair were rare (39 cases), and therefore simply not taken into account if they contradicted each other.

\subsection{Quality Indicators}
\label{sec_qualityindicators}
The evaluation focuses on the quality of the following three main aspects of our algorithm:
\begin{enumerate}
\item Retrieval of matching images
\item Semantic hierarchy and clusters\index{Semantic Clustering}
\item Visual clustering\index{Visual Clustering}
\end{enumerate}

We measure the quality of the image retrieval (\emph{1.}) by calculating the precision, recall, and F-Measure for the  pictures returned by the algorithm compared to the crowdsourced test set.

The quality of the hierarchy of the retrieved images (\emph{2.}) is based on the \emph{same object} and \emph{same object and context} pairs: The  minimal path distance for an annotated pair of pictures is calculated and used to determine the closeness of two images: \[closeness(x,y) = 1/distance(x,y)\]
Averaging this value over all pairs of a similarity category returns a value between 0 and 1. Optimal values are 1 for positive (similar) pairs, and 0 for negative (non-similar) pairs. The similarity categories are below referenced as $c_o$ for same object pairs, $c_c$ for same object and same context pairs, and $c_n$ for not similar pairs.
The keyword clustering is included in this evaluation step by treating the clusters in a node as its children. Consequentially, the perfect score of 1 can only be reached when two semantically similar images are not only in the same node but also in the same semantic subcluster.

\bigskip
As described in chapter \ref{sec_visualclustering}, the semantic clusters are divided again into smaller visual clusters.
To test the performance of this visual subclustering (\emph{3.}), the clusters formed by the algorithm are compared to the test data from the second phase, by analyzing which of the images annotated as \emph{visually not similar} are actually wrongly assigned to the same visual clusters. This is expressed through precision and recall.
The test set contains 10,894 associations annotated as \emph{visually not similar} and 1,841 as \emph{visually similar} annotated image pairs.
Since there are approximately 1.3 million possible combinations between two images within the images annotated as food, this is an approximate 1\% coverage, meaning we know only for every 100th combination, whether or not it is considered visually similar.
Since few of those pairs will be within the same node, it is impossible to gain any valuable information on the subclustering we do within the semantic clusters.

In order to evaluate the visual clustering nevertheless, this clustering was performed on all retrieved images, without considering the tree structure or semantic clusters.
The images the algorithm provides for the search term ``food'' were also filtered to take only those into account that were claimed to show food. 883 images remained.
This way, we prevent to visually cluster irrelevant images.\\
Since k-means\index{K-Means} is used as an algorithm to cluster images by their visual features, and it is based on a random distribution, we performed ten measurements to reduce the error.

\subsection{Results}
\label{sec_results}

\subsubsection{Image Retrieval}

Our image retrieval has a precision of 50.2\% and recall of 85.9\% on the query ``food'', before execution of the semantic clustering that removes outliers. Without the use of co-occurring tags described in section \ref{sec_picturestonodes}, both values show no significant difference with p = 50.5\% and r = 85.4\%.\\
After the semantic clustering, the measures depend on the  parameters \emph{minimal node size}, \emph{mcl clustering threshold}, and \emph{minimal mcl cluster size}, described in sections \ref{sec_picturestonodes} and \ref{sec_keywordclustering}. The results for different values of this parameters are presented in table \ref{tab_retrievalevaluation}.\\

\begin{table}[h]
   \begin{tabular}{| p{2.2cm}| p{2.2cm}| p{2cm} || p{2cm} | p{2cm} | p{2cm} |}
    \hline
    \emph{mcl clustering threshold} & \emph{minimal mcl cluster size} & \emph{minimal node size} & \emph{precision} & \emph{recall} & \emph{f-measure} \\ \hline
    0 	& 0 	& 0 & 0.501532 & 0.859143 & 0.633344 \\ \hline
    0 	& 5 	& 0 & 0.559668 & 0.783036 & 0.652773 \\ \hline
    5 	& 5 	& 5 & 0.549815 & 0.798214 & 0.651129 \\ \hline
    15 	& 25 &  5 & 0.615894 & 0.747321 & 0.675272 \\ \hline
    15 	& 10 & 15 & 0.585333 & 0.783929 & 0.670229 \\ \hline
    15 	& 25 & 15 & 0.695298 & 0.672791 & 0.683859 \\ \hline
    	100 	& 100 & 100 & 0.757858 & 0.569554 & 0.650350 \\ \hline
    \end{tabular}
    \caption{Precision and recall of the image retrieval}
	\label{tab_retrievalevaluation}
\end{table}


\subsubsection{Semantic Hierarchy and Clusters}

\index{Semantic Clustering}
The results of the semantic hierarchy and cluster evaluation also depend on the parameters mentioned for image retrieval. The measurements listed in table \ref{tab_treeevaluation} indicate that the best distinction between images showing the same objects and images showing different objects is achieved with low \emph{minimal mcl cluster size}, that is, without outlier removal. The other parameters' values correlate with those of the image retrieval evaluation above.\\

\begin{table}[h]
    \begin{tabular}{| p{2.2cm} | p{2.2cm} | p{2cm} || p{1.4cm} | p{1.4cm} | p{1.4cm} | p{1.4cm} |}
	\hline
    \emph{mcl clustering threshold} & \emph{minimal mcl cluster size}		& \emph{minimal node size} & $c_o $ & $c_c$ & $c_n$ & $c_o-c_n$ \\ \hline
    0 	& 0 	& 0 & 0.25075 & 0.25483 & 0.23430 & 0.01645 \\ \hline
    0 	& 5 	& 0 & 0.25707 & 0.26691 & 0.24857 & 0.00850 \\ \hline
    5 	& 5 	& 5 & 0.26129 & 0.27160 & 0.25347 & 0.00782 \\ \hline
    15 	& 25 &  5 & 0.24118 & 0.24927 & 0.23345 & 0.00773\\ \hline
    15 	& 0 & 15 & 0.27757 & 0.28242 & 0.25897 & 0.01860 \\ \hline
    15 	& 10 & 15 & 0.28285 & 0.29194 & 0.26884 & 0.01401 \\ \hline
    15 	& 25 & 15 & 0.28571 & 0.29391 & 0.27563 & 0.01008 \\ \hline
    	100 	& 100 & 100 & 0.32578 & 0.34126 & 0.31711 & 0.00867 \\ \hline
    \end{tabular}
    \caption{Semantic quality measures}
	\label{tab_treeevaluation}
\end{table}

Varying the parameters most strongly influences the amount of the closeness measures, that is, all their values rise or drop somewhat consistently.


\subsubsection{Visual Clustering}

\index{Visual Clustering}
The 883 images, which are identified as ``food'' by both the algorithm and the test set creation participants, are clustered into 21 visual clusters (see chapter \ref{sec_visualclustering}). The average precision is 84.7\% with a recall of 93.5\%, which leads to an F-Measure of 88.9\%.

Since our visual clustering algorithm is supposed to be executed on already formed semantic clusters, it is intended to be used on smaller sets of images. A second measurement is performed with 100 randomly picked images which are then clustered 100 times into 7 buckets. Here, a precision of 87.9\%  is achieved while the recall falls to 82.2\%. The F-Measure is 84.9\%.


\iffalse{begin comment}
Analysis data:

100 images, 7 visual clusters (average over 100x):
  Testset contains  1841 visually similar   image tuples
  And there are    10894 visually different image tuples

  Similar   images, average true  negatives: 3.140000
  Similar   images, average false positives: 9.860000
  Different images, average true  positives: 71.490000
  Different images, average false negatives: 15.510000

  Precision: 0.878795 (tp / (tp + fp)) = 71.49/81.35
  Recall:    0.821724 (tp / (tp + fn))
  F-Measure: 0.849302 (2 * (p * r / (p + r)))

all 883 images, 21 visual clusters (average over 10x):
  Testset contains  1841 visually similar   image tuples
  And there are    10894 visually different image tuples

  Similar   images, average true  negatives: 95.800000
  Similar   images, average false positives: 1086.200000
  Different images, average true  positives: 6012.100000
  Different images, average false negatives: 417.900000

  Precision: 0.846977 (tp / (tp + fp))
  Recall:    0.935008 (tp / (tp + fn))
  F-Measure: 0.888818 (2 * (p * r / (p + r)))
\fi



