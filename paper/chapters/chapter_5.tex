%
\section{Evaluation}
\label{sec_evaluation}

We evaluated our tool on a set of 9,201 images and the query term ``food''. Since no comparable algorithms exist, the evaluation is mainly aimed at obtaining the best values for the parameters and at providing a basis for comparison of further improvements and future work.

\subsection{Testset}
\label{sec_testset}
No gold standard is available to tell us which pictures show food and how similar the images are. The creation of such standards and training data is exactly the task we want to facilitate with this work.\\ 
What we did to receive evaluation data was to crowdsource the needed data from the general public. This was achieved in two phases:\\

First, the users were shown random picture out of the 9,201 test set images and asked whether it shows food. We normalized these answers, so that there is only one vote per user per picture, shoe value is determined by the ration of positive (``shows food'') and negative (``does not show food'') votes of that user on that picture. We consider all those images as showing food that received at least 50\% positive votes. With over 35,000 clicks by more than 40 participants, 1,142 images were identified to show food. \\

We also need data on the semantic and visual similarity of the pictures. Therefore, in the second phase, the users were shown pairs of images and asked to compare them. They could choose between three levels of semantic similarity: \emph{not similar}, \emph{same object}, and \emph{same object and same context}, and two levels of visual similarity: \emph{similar}, and \emph{not similar}.\\
Among the 12,962 votes were 771 pairs of images with same objects, 354 pairs with same object and same context, as well as 1,885 pairs of visually similar images. The same normalization as in the first phase was applied.

\subsection{Quality Indicators}
The evaluation focusses on the following four main aspects of our algorithm:
\begin{enumerate}
\item Retrieval of matching images
\item Semantic hierarchy of retrieved images
\item Semantic clustering
\item Visual clustering
\end{enumerate}

We measure the quality of the image retrieval (1.) by precision and recall of returned pictures, compared to those that were declared to show food by the test persons. \todo{should we compare synset detection mechanisms?} 

The quality of the hierarchy of the retrieved images (2.) is based on the same object and same object and context pairs: The  minimal path distance for an annotated pair of pictures can be calculated and used to determine the closeness of two images $closeness(x,y) = 1/distance(x,y)$. Averaging this value over all pairs of a similarity category returns a value between 0 and 1 (below referenced as $c_o$ for same object pairs, $c_c$ for same object and same context pairs, and $c_n$ for not similar pairs), with the optimal values being 1 for positive (similar) pairs, and 0 for negative (non-similar) pairs.

The same object and same context annotations can also be used to examine the quality of the keyword clusters (3.).  (compare both, what does mcl actually do?)\todo{how do we actually evaluate mcls?}

Visual similarity (4.) is evaluated on the whole testset, because not enough comparison data is available to get valuable results if only comparisons within semantic clusters were used. Once again, precision and recall are used as indicators.

vary parameters given by frontend, trying to find best configuration

\subsection{Results}
\label{sec_results}
Our image retrieval has a precision of 0.50 and recall of 0.86 on the ``food'' query, before execution the semantic clustering that removes outliers. Without the use of co-occurring tags described in section \ref{sec_picturestonodes}, both values changed slightly to p = 0.?? and r = 0.??.\todo{get actual values}
After the semantic clustering, the measures depend on the \emph{minimal mcl cluster size} parameter. The results for different values of this parameter are presented in table \ref{tab_retrievalevaluation}.\\

\begin{table}[h]
    \begin{tabular}{| p{4.5cm} || p{1.5cm} | p{1.5cm} |}
    \hline
    \emph{minimal mcl cluster size} & \emph{precision} & \emph{recall} \\ \hline
    value 1 		& value 1 	& value 1 \\ \hline
    value 2 		& value 2 	& value 2 \\ \hline
    value 3	 	& value 3 	& value 3 \\
    \hline
    \end{tabular}
    \caption{Precision and recall of the image retrieval}
	\label{tab_retrievalevaluation}
\end{table}


\begin{table}[h]
    \begin{tabular}{| p{1.8cm} | p{1.8cm} || p{1.5cm} | p{1.5cm} | p{1.5cm} |}
    \hline
    ?	& \emph{minimal node size} & $c_o $ & $c_c$ & $c_n$ \\ \hline
    value 1 		& value 1 	& value 1 	& value 1	& value 1 \\ \hline
    value 2 		& value 2	& value 2 	& value 2	& value 2 \\ \hline
    value 3	 	& value 3	& value 3 	& value 3	& value 3 \\
    \hline
    \end{tabular}
    \caption{The two main phases of our algorithm}
	\label{tab_treeevaluation}
\end{table}

