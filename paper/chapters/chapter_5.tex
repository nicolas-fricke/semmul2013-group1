%
\section{Evaluation}
\label{sec_literatur}

\subsection{Testset}
receive testset from users through web-based tool, crowdsource in two steps: \\

\begin{enumerate}
\item random picture, does it show food? identified 1270 images out of 9202 as showing food with over 35000 clicks, where those with at least 50 percent of positives votes are considered to show food (normalized so that one vote per evaluator, value is this persons relation of positive and negative votes on this pictures) \\
\item For subset of those (300 images), compare two pictures: semantically not similar / same object / same object and same context? visually similar / not similar? However, we limited the set of pictures for this step to 300 in order to have a feasible amount of necessary comparisons.\\
\end{enumerate}

\subsection{Evaluation Method}
how are quality measures calculated? \\

search food: precision  / recall of picture inclusion (compare synset detection mechanisms?) \\
evaluate tree nodes based on same object / same context annotations: average minimal path distance for annotated pairs of pictures, use to calculate similarity $1/distance$. Optimal: 1 for positive pairs, 0 for negative pairs\\
evaluate mcl clusters based on same object and on same context annotations (compare both, what does mcl actually do?)\todo{how do we actually evaluate mcls?} \\

We evaluate visual similarity on the whole testset, because not enough comparison data to get valuable results if we only use comparisons within semantic clusters. Calculate precision and recall\\

vary parameters given by frontend, trying to find best configuration \\

\subsection{Results}

\missingfigure{table}