%
\section{Results Discussion}
\label{sec_discussion}

\todo{Are our results good? Are they biased by something?}
It can generally be said that the quality of the results highly depends on the original image annotations: an inappropriate tag leads the algorithm to ``believe'' that the picture shows something that is actually not present.

\subsection{Test Set Quality}
The evaluation results also depend on the test set, which, unfortunately, cannot be clearly right or wrong. Different users will expect different images to be returned according to their definition of food: When some of the participants of the test set creation were asked which items they considered food, the answers ranged from ``Those that I would like to eat'' to ``Anything that some living organism would eat''. \\
Another problem lies in the fact that pictures often contain small or processed items, which makes it hard to identify the exact contents of that picture. That's why during test set creation participants could probably not see the described content or interpret the image different in contrast to the original tags of the image. \todo{phrase this better or remove}
It also has to be assumed that people have different opinions on what images are visually similar, especially since no definition or hints were given to the participants. We used crowdsourcing to deal with these problems and obtain a test set that is supported by the majority of users. So the key question to the quality of the test set is whether there are enough participants to obtain a representative result.

\subsection{Image Retrieval}
One of the reasons for the generally poor precision of the image retrieval may lie in poorly annotated images.
Other, more controllable reasons, whatsoever, are to be searched in the Synset detection mechanism. First, the limitation to nouns leads to incorrectly identified Synsets because adjectives, adverbs and verbs are wrongly matched to nouns if such exist. 
Second, words in other languages than English may be incorrectly matched if they exist in a different meaning in English.\todo{Examples for each reason!}
And third, the assumption that tags on the same picture are semantically close does not hold in all cases. \\
The first two reasons can be addressed by using a more sophisticated ontology and similarity measure, but the third requires a rethinking of the Synset detection algorithm.

\subsection{Semantic Clusters}
Our semantic evaluation method presents a good method to get an overview of the semantic clustering in the complete search tree. But we also have to consider the inner dependencies of the semantic clusters. The result of the MCL based clusters highly depend on the quality of their parent keyword clusters. That's why MCL clusters are hard to evaluate, because they cannot be isolated from their dependent keyword cluster.\\

\subsection{Visual Clusters}
also rather hard to look at in isolation, because method specifically designed for final subclustering. But lack of data for evaluation within subclusters for appropriately sized semantic clusters