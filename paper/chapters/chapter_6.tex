%
\section{Results Discussion}
\label{sec_discussion}

It can generally been said that the quality of the results highly depends on the original image annotations: an inappropriate tag leads the algorithm to ``believe'' that the picture shows something that is actually not present.

\subsection{Test Set Quality}
The evaluation results also depend on the test set, which, unfortunately, cannot be clearly right or wrong. Different users will expect different images to be returned according to their definition of food: When some of the participants of the test set creation were asked which items they considered food, the answers ranged from ``Those that I would like to eat'' to ``Anything that some living organism would eat''. \\
Another problem lies in the fact that pictures often contain small or processed items, which makes it hard to identify the exact contents of that picture. That's why during test set creation participants could probably not see the described content or interpret the image different in contrast to the original tags of the image.
It also has to be assumed that people have different opinions on what images are visually similar, especially since no definition or hints were given to the participants. We used crowdsourcing to deal with these problems and obtain a test set that is supported by the majority of users. So the key question to the quality of the test set is whether there are enough participants to obtain a representative result.

\subsection{Image Retrieval}
One of the reasons for the generally poor precision of the image retrieval may lie in poorly annotated images.
Other, more controllable reasons, whatsoever, are to be searched in the Synset detection mechanism.
First, the limitation to nouns leads to incorrectly identified Synsets because adjectives, adverbs and verbs are wrongly matched to nouns if such exist, e.g. \emph{fall} as a verb for falling under the influence of gravity and as a noun for autumn.
Second, words in other languages than English may be incorrectly matched if they exist in a different meaning in English, e.g. \emph{gift} for present in English and poison in German.
And third, the assumption that tags on the same picture are semantically close does not hold in all cases (like words with different meanings also known as homographs), e.g. a \emph{cherry} on a plate made of \emph{wood} would be assigned to the cherry tree meaning instead of cherry as a fruit. \\
The first two reasons can be addressed by using a more sophisticated ontology and similarity measure, but the third requires a rethinking of the Synset detection algorithm.

\bigskip
Key points on MCL: generally useful (removes more non-food images than food images, else recall would go down with precision staying the same), interestingly best results when minimal mcl cluster size larger than minimal node size (i.e. small nodes are deleted anyways)

\subsection{Semantic Clusters}	
The closeness of nodes shows, that images, which were annotated as semantically similar, are in the tree closer to each other than as semantically not similar annotated ones. However, all values are rather close to average tree distance. Furthermore, the varying of parameters influences only the average tree distance, not the difference between distances. In our opinion, pictures, annotated as showing same object, should at least be in the same node, which means closeness should have value greater than 0.5. Images with same context should be in the same cluster, therefore, closeness should be 1. Unfortunately the results are best with other parameters than for a good image retrieval.

\bigskip
One problem, which we encountered, is the difficulty of defining ``semantically similar''. The second phase of our evaluation process included new participants, who did not know that all images contain food. That is why, some participants voted different kinds of food as same object. Others voted images as showing same object, only if they really showed the same object (both showing a strawberry). This is a problem of granularity, depending on how fine grained participants evaluated semantic similarity, the results differ. Participants later stated insecurity about these votes and own inconsistency. A solution could be a detailed description of semantic categories, but our aim was to achieve a quite intuitive clustering. For a significant evaluation, more participants are needed. 

\bigskip
Our semantic evaluation method is a good method to get a quality measure of the complete semantic hierarchy. However, this makes it hard to make statements about the individual semantic clustering steps. For example, the results of the keyword based clustering highly depend on the quality of the calculated keyword clusters. Those are hard to judge in isolation, though.\\	

\subsection{Visual Clusters}

The visual clusters are also difficult to evaluate in isolation, because we chose our approach for the use case of refining the semantic clusters. The method was explicitly designed to build subclusters on small image sets (about 5 to 50 images). As stated in section \ref{sec_qualityindicators}, the lack of comparison data rendered it impossible to perform the evaluation on the previously calculated semantic clusters.
\begin{enumerate}
\item it's also hard for humans to differentiate more than ...7?... visual clusters based on color and edginess, if we now have > 20 visual clusters, some might be quite similar for humans => many false negatives
\item we still only have few votes on visually similar images, the percentage of not similar votes outnumbers the similar votes by almost factor 10. this way, we have few hits for true positives.
\item users may have quite different opinions on when two images are regarded to be visually similar
\item testset generation always showed two images and asked "similar or not", there was no way to say two images are more similar than two others (which may still be sort of similar)
\end{enumerate}
