%
\section{Results Discussion}
\label{sec_discussion}

The results obtained in the evaluation were partly expected, but also partly surprising. The following sections give an insight into the reasons for unexpected and remarkable results.

\subsection{Test Set Quality}
The evaluation results depend on the test set, which, unfortunately, cannot be clearly right or wrong. Different users will expect different images to be returned according to their definition of food: When some of the participants of the test set creation were asked which items they considered food, the answers ranged from ``Those that I would like to eat'' to ``Anything that some living organism would eat''. \\
Another problem lies in the fact that pictures often contain small or processed items, which makes it hard to identify the exact contents of that picture. That is why, during test set creation participants could probably not see the described content or interpret the image different in contrast to the original tags of the image.
It also has to be assumed that people have different opinions on what images are visually similar, especially since no definition or hints were given to the participants. We used crowdsourcing to deal with these problems and obtain a test set that is supported by the majority of users. So the key question to the quality of the test set is whether there are enough participants to obtain a representative result.

\subsection{Image Retrieval}
One of the reasons for the generally poor precision of the image retrieval may lie in poorly annotated images.
Other, more controllable reasons, whatsoever, are to be searched in the Synset\index{Synset} detection mechanism.
First, the limitation to nouns leads to incorrectly identified Synsets because adjectives, adverbs and verbs are wrongly matched to nouns if such exist, e.g. \emph{fall} as a verb for falling under the influence of gravity and as a noun for autumn.
Second, words in other languages than English may be incorrectly matched if they exist in a different meaning in English, e.g. \emph{gift} for present in English and poison in German.
And third, the assumption that tags on the same picture are semantically close does not hold in all cases (like words with different meanings also known as homographs), e.g. a \emph{cherry} on a plate made of \emph{wood} would be assigned to the cherry tree meaning instead of cherry as a fruit. \\
The first two reasons can be addressed by using a more sophisticated ontology\index{Ontology} and similarity measure, but the third requires a rethinking of the Synset detection algorithm.

\bigskip
The evaluation also indicates the effectiveness of an additional semantic clustering via keyword clusters for the image retrieval. One assumption was, that if only few pictures are assigned to the same keyword cluster, they can be declared as outliers and therefore be deleted from the retrieval result. The parameter \emph{minimal mcl cluster size} determines which clusters are deleted. The results show that increasing the \emph{minimal mcl cluster size} parameter in fact also increases the F-Measure. The best results are achieved if the \emph{minimal mcl cluster size} is greater than \emph{minimal node size}. That means deleting pictures is more effective than merging with parent node.

\subsection{Semantic Clusters}	
\index{Semantic Clustering}
The closeness of nodes shows, that images, which were annotated as semantically similar, are in the tree closer to each other than as semantically not similar annotated ones. However, all values are rather close to average tree distance. Furthermore, the varying of parameters influences only the average tree distance, not the difference between distances. In our opinion, pictures, annotated as showing same object, should at least be in the same node, which means closeness should have value greater than 0.5. Images with same context should be in the same cluster, therefore, closeness should be 1. Unfortunately the results are best with other parameters than for a good image retrieval.

\bigskip
One problem, which we encountered, is the difficulty of defining ``semantically similar''. The second phase of our evaluation process included new participants, who did not know that all images contain food. That is why, some participants voted different kinds of food as same object. Others voted images as showing same object, only if they really showed the same object (both showing a strawberry). This is a problem of granularity, depending on how fine grained participants evaluated semantic similarity, the results differ. Participants later stated insecurity about these votes and own inconsistency. A solution could be a detailed description of semantic categories, but our aim was to achieve a quite intuitive clustering. For a significant evaluation, more participants are needed.

\bigskip
Our semantic evaluation method is a good method to get a quality measure of the complete semantic hierarchy. However, this makes it hard to make statements about the individual semantic clustering steps. For example, the results of the keyword based clustering highly depend on the quality of the calculated keyword clusters. Those are hard to judge in isolation, though.\\	

\subsection{Visual Clusters}
\index{Visual Clustering}
The visual clustering shows good results in the evaluation, i.e. the clustering puts images a human considers visually different in the majority of cases into different visual clusters. Yet, in the case of 883 images, our algorithm separates the pictures in 23 different clusters, which may be a lot for humans to differentiate. Fortunately, the visual clustering is intended to run on smaller image sets, since the image sets are already semantically pre-clustered. 

\bigskip
When performing the analysis on a smaller subset with 100 images, the precision went up. Additionally, the images were clustered in only 7 clusters. But, the recall decreases , because less clusters lead to less differentiation. \todo{one sentence why falling recall not bad, see last point in notes}

\iffalse{deprecated}
\begin{enumerate}
\item Already quite good results, with precision and recall both > 80\%
\item This means our algorithm is good in putting images a human considers visually different into different visual clusters
\item We tested initially on 883 images and got 23 clusters, which may be quite a lot for humans, since it may be hard to think of more than 10 different clusters of color and "edginess" combinations. Number of clusters is determined by "rule of thumb"
\item Good thing: our algorithm is intended to run on smaller image sets, since these are already semantically pre-clustered.
\item When performing analysis on smaller subset with 100 images (still quite big, our sets are rather 10-20 images! but then we have to little reference id tuples in test set) the precision went up to an astonishing > 87\%!
\item The 100 images were clustered into 7 clusters. That's great, because for a human it may be easier to differentiate between these clusters (=> the one may be black and white with hard edges, the other rather bluish with smooth curves). Naturally this way, it is more likely that images our test set users considered different wind up in the same visual cluster (since there are fewer, they are less fine grained)
\item This way the recall went "down" to 82\%, which still is a very good value!
\item Clustering is always a difficult task for humans: It's easy to compare two images and say if they are similar or not, but it's hard to draw a border (1 is equal to 2, 2 to 3, 3 to 4. But 1 is not equal to 4. where to draw the line?) - also true for semantic clustering
\item Since it is likely to be easier for users to understand the now fewer clusters (the borders / differences between the clusters), this recall dip is acceptable
\item We're awesome! :)
\end{enumerate}
\fi