%
\section{Results Discussion}
\label{sec_discussion}

It can generally been said that the quality of the results highly depends on the original image annotations: an inappropriate tag leads the algorithm to ``believe'' that the picture shows something that is actually not present.

\subsection{Test Set Quality}
The evaluation results also depend on the test set, which, unfortunately, cannot be clearly right or wrong. Different users will expect different images to be returned according to their definition of food: When some of the participants of the test set creation were asked which items they considered food, the answers ranged from ``Those that I would like to eat'' to ``Anything that some living organism would eat''. \\
Another problem lies in the fact that pictures often contain small or processed items, which makes it hard to identify the exact contents of that picture. That's why during test set creation participants could probably not see the described content or interpret the image different in contrast to the original tags of the image. 
It also has to be assumed that people have different opinions on what images are visually similar, especially since no definition or hints were given to the participants. We used crowdsourcing to deal with these problems and obtain a test set that is supported by the majority of users. So the key question to the quality of the test set is whether there are enough participants to obtain a representative result.

\subsection{Image Retrieval}
One of the reasons for the generally poor precision of the image retrieval may lie in poorly annotated images.
Other, more controllable reasons, whatsoever, are to be searched in the Synset detection mechanism.
First, the limitation to nouns leads to incorrectly identified Synsets because adjectives, adverbs and verbs are wrongly matched to nouns if such exist, e.g. \emph{fall} as a verb for falling under the influence of gravity and as a noun for autumn. 
Second, words in other languages than English may be incorrectly matched if they exist in a different meaning in English, e.g. \emph{gift} for present in English and poison in German.
And third, the assumption that tags on the same picture are semantically close does not hold in all cases (like words with different meanings also known as homographs), e.g. a \emph{cherry} on a plate made of \emph{wood} would be assigned to the cherry tree meaning instead of cherry as a fruit. \\
The first two reasons can be addressed by using a more sophisticated ontology and similarity measure, but the third requires a rethinking of the Synset detection algorithm.

\bigskip
Key points on MCL: generally useful (removes more non-food images than food images, else recall would go down with precision staying the same), interestingly best results when minimal mcl cluster size larger than minimal node size (i.e. small nodes are deleted anyways)

\subsection{Semantic Clusters}
closeness confirms idea that semantically close images are closer to each other in the tree than semantically different ones
however, all values seem rather close to average tree distance. average is also what mostly varies with mcl parameters
same objects should at least be in same node -> closeness >= 0.5; same context also in same cluster -> closeness 1.0
major reason: testset. some  users only participated in second phase, did not know all images contained food already (so voted very different kinds of food as same). but also for others, since no definition of 'same object' given, difficult choice of level of abstraction / granularity (are cake and cupcake the same? cake and cookie?). participants later stated insecurity about these votes and own inconsistency when no pairs appeared they considered same.\\
unfortunately for tool, best parameters for recall/precision do not correlate with best parameter values for semantic similarity 
\bigskip
Our semantic evaluation method is a good method to get an quality measure of the complete semantic hierarchy. However, this makes it hard to make statements about the individual semantic clustering steps. For example, the results of the keyword based clustering highly depend on the quality of the calculated keyword clusters. Those are hard to judge in isolation, though.\\	


\subsection{Visual Clusters}
also rather hard to look at in isolation, because method specifically designed for final subclustering. But lack of data for evaluation within subclusters for appropriately sized semantic clusters
